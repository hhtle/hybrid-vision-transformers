{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b7692bb",
   "metadata": {},
   "source": [
    "# Import and prepare data from MNIST and CIFAR-10\n",
    "\n",
    "### Packages import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee8fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b08217",
   "metadata": {},
   "source": [
    "### Configs\n",
    "\n",
    "All configs files are in hybrid-vit/configs. We can tune a lot of parameters in here. A config file will corespond to one optimization procedure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afafcc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: mnist\n",
      "root: ./data/cache\n",
      "img_size: 28\n",
      "augment: False\n",
      "batch_size: 128\n",
      "num_workers: 4\n",
      "seed: 17092003\n",
      "val_size: 5000\n"
     ]
    }
   ],
   "source": [
    "def load_cfg(path):\n",
    "    # Load a YAML config file into a Python dict\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "    \n",
    "\n",
    "cfg = load_cfg(\"configs/mnist_baseline.yaml\")\n",
    "dl_cfg = {\n",
    "        \"name\": cfg[\"dataset\"][\"name\"],\n",
    "        \"root\": cfg[\"dataset\"][\"root\"],\n",
    "        \"img_size\": cfg[\"dataset\"][\"img_size\"],\n",
    "        \"augment\": cfg[\"dataset\"].get(\"augment\", True),\n",
    "        \"batch_size\": cfg[\"optim\"][\"batch_size\"],\n",
    "        \"num_workers\": max(4, os.cpu_count() - 4),\n",
    "        \"seed\": cfg[\"misc\"][\"seed\"],\n",
    "        \"val_size\": 5000,\n",
    "    }\n",
    "\n",
    "for key, value in dl_cfg.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c99110",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Use file build_dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1106a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta: {'num_classes': 10, 'image_size': 28, 'channels': 3, 'train_len': 55000, 'val_len': 5000, 'test_len': 10000}\n",
      "img_size = 28, patch = 4, expected tokens = 49\n",
      "batch shapes: torch.Size([128, 3, 28, 28]) torch.Size([128])\n",
      "dtype/range: torch.float32 [-0.424, 2.821]\n"
     ]
    }
   ],
   "source": [
    "from data.dataloaders import build_dataloaders\n",
    "train_loader, val_loader, test_loader, meta = build_dataloaders(dl_cfg)\n",
    "\n",
    "# Compute the expected number of ViT tokens from img_size and patch\n",
    "patch = cfg[\"dataset\"][\"patch\"]       # used by the model, not the DataLoader\n",
    "img_size = cfg[\"dataset\"][\"img_size\"]\n",
    "assert img_size % patch == 0, \"img_size must be divisible by patch\"\n",
    "num_tokens = (img_size // patch) ** 2\n",
    "\n",
    "print(\"meta:\", meta)\n",
    "print(f\"img_size = {img_size}, patch = {patch}, expected tokens = {num_tokens}\")\n",
    "\n",
    "# Pull one batch to verify shapes and value range\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"batch shapes:\", xb.shape, yb.shape)  # expected: [B, 3, H, W], [B]\n",
    "print(\"dtype/range:\", xb.dtype, f\"[{xb.min().item():.3f}, {xb.max().item():.3f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
